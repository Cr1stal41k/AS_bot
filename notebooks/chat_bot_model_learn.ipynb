{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhwMHLgjz-zJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# AS chat bot model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bleValxw0FhZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Description\n",
    "This notebook contains chat-bot implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6wso_OK0lpL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xMNY6q-mYGt6",
    "outputId": "45685ab9-29b5-4fbd-e294-6ea411618454",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from pyxdameraulevenshtein import normalized_damerau_levenshtein_distance_seqs\n",
    "# Library not installed, unable to resolve dependencies for this project\n",
    "# import pyaspeller\n",
    "import pymorphy2\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words([\"russian\", \"english\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_rhIhbN09qn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Services functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmBtvn8HyVtL",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_garbage(raw_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removing garbage (any characters not from the Russian and English alphabets, numbers a etc.) from text.\n",
    "    :param raw_text: text to be processed. \n",
    "    :return: processed text containing only letters and spaces.\n",
    "    \"\"\"\n",
    "    return re.sub('[^А-Яа-яA-Za-z- ]', '', raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLR1EIXayxqz",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(raw_text: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenizing text.\n",
    "    :param raw_text: text to be processed.\n",
    "    :return: list with words.\n",
    "    \"\"\"\n",
    "    raw_text = raw_text.lower()\n",
    "    tokens = nltk.word_tokenize(raw_text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4F6fKLD6e9BJ",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# def correct_orthography(sentence: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Checks and corrects spelling errors and typos in sentences.\n",
    "#     :param sentence: sentence to check.\n",
    "#     :return: corrected sentence\n",
    "#     \"\"\"\n",
    "#     speller = pyaspeller.YandexSpeller()\n",
    "#     changes = {change[\"word\"]: change[\"s\"][0] for change in speller.spell(sentence)}\n",
    "#     for word, suggestion in changes.items():\n",
    "#         sentence = sentence.replace(word, suggestion)\n",
    "#     return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def fix_typos(word: str, words: list) -> str:\n",
    "    \"\"\"\n",
    "    Checks and corrects word errors.\n",
    "    :param word: word to check.\n",
    "    :param words: the dictionary against which the check will be carried out.\n",
    "    :return: if word contains errors > 45% - return uncorrected word, else - corrected word.\n",
    "    \"\"\"\n",
    "    array = np.array(words)\n",
    "    result = list(zip(words, list(normalized_damerau_levenshtein_distance_seqs(word, array))))\n",
    "\n",
    "    command, rate = min(result, key=lambda x: x[1])\n",
    "    \n",
    "    if rate > 0.45:\n",
    "        return word\n",
    "        \n",
    "    return command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "141f6O1YyzsA",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(tokenized_text: list) -> list:\n",
    "    \"\"\"\n",
    "    Removing stop words from tokenized text.\n",
    "    :param tokenized_text: list, that contains tokens.\n",
    "    :return: list, without stop words.\n",
    "    \"\"\"\n",
    "    filtered_tokens = [word for word in tokenized_text if word not in stop_words]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81tA56GJy1hQ",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def to_base_form(raw_text: list) -> list:\n",
    "    \"\"\"\n",
    "    Brings the words back to its base form.\n",
    "    :param raw_text: raw text, which needs to be processed.\n",
    "    :return: list in which words are reduced to their base form.\n",
    "    \"\"\"\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    base_form = []\n",
    "    for word in raw_text:\n",
    "        if len(word) < 2:\n",
    "            continue\n",
    "        word = morph.parse(word)[0]\n",
    "        base_form.append(word.normal_form)\n",
    "    return base_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZJO2FuSbNkq",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def word_processing(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Function, which unites everything related to word processing.\n",
    "    :param text: raw text.\n",
    "    :return: list with processed words.\n",
    "    \"\"\"\n",
    "    text = remove_garbage(text)\n",
    "    # text = correct_orthography(text)\n",
    "    text = tokenize(text)\n",
    "    text = to_base_form(text)\n",
    "    result = remove_stop_words(text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uppzz_ag4z3u",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVAxvOtt5nNQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We create 3 arrays: documents, classes and words and fill them with data from the bot config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6e_bax3Y0B05",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from packages.loaders import config\n",
    "from packages.path_storage.path_storage import PathStorage\n",
    "\n",
    "%%time\n",
    "words = []\n",
    "intents = []\n",
    "documents = []\n",
    "\n",
    "for intent in config[\"intents\"]:\n",
    "    for pattern in config[\"intents\"][intent][\"patterns\"]:\n",
    "        word = word_processing(pattern)\n",
    "\n",
    "        words.extend(word)\n",
    "        documents.append((word, intent))\n",
    "\n",
    "        if intent not in intents:\n",
    "            intents.append(intent)\n",
    "\n",
    "words = sorted(list(set(words)))\n",
    "intents = sorted(list(set(intents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lcfALL043_Wh",
    "outputId": "04ba014f-98b2-4011-a1c3-786e57c022ca",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(len(documents), \"documents\")\n",
    "print()\n",
    "print(len(intents), \"intents\", intents)\n",
    "print()\n",
    "print(len(words), \"unique lemmatized words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(words, open(PathStorage.get_path_to_models() / 'words.pkl', 'wb'))\n",
    "pickle.dump(intents, open(PathStorage.get_path_to_models() / 'intents.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptkWj6fr5-YD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Creating a training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mw3dhrXj68mH",
    "outputId": "0412b4b6-9906-4d34-ea50-9855ec3445d4",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "training = []\n",
    "output_empty = [0] * len(intents)\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    pattern_words = doc[0]\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    output_row = list(output_empty)\n",
    "    output_row[intents.index(doc[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# create train and test lists. X - patterns, Y - intents\n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])\n",
    "\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlkKp08N6E6b",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PM3F6AoOGjt9",
    "outputId": "f159c39a-cf07-4988-fb9b-838cd2be77b2",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(train_x, train_y, epochs=30, batch_size=5, verbose=1, workers=-1)\n",
    "model.save(PathStorage.get_path_to_models() / \"perceptron.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIJvGccp6TKK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Chat-bot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmyiMQsNHg__",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def bow(sentence: str, words: list) -> np.array:\n",
    "    \"\"\"\n",
    "    Processing the user's text and comparing the received words with the bot's dictionary.\n",
    "    :param sentence: user's text.\n",
    "    :param words: chat bot dictionary.\n",
    "    :return: an array the size of the number of all words in the chat bot config, where 0 and 1 denote whether there is a word in the user's text or not.\n",
    "    \"\"\"\n",
    "    prepared_data = word_processing(sentence)\n",
    "    prepared_data_fixed_typos = []\n",
    "    for word in prepared_data:\n",
    "      word = fix_typos(word, words)\n",
    "      prepared_data_fixed_typos.append(word)\n",
    "    bag = [0] * len(words)\n",
    "    for s in prepared_data_fixed_typos:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "    return np.array(bag)\n",
    "\n",
    "\n",
    "def predict_intent(sentence: str, model) -> dict or None:\n",
    "    \"\"\"\n",
    "    User's text predicts his intention.\n",
    "    :param sentence: user's text.\n",
    "    :param model: chat bot model.\n",
    "    :return: list with a dictionary, which indicates the most probable intention and its probability.\n",
    "    \"\"\"\n",
    "    p = bow(sentence, words)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = config[\"threshold\"]\n",
    "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    if len(results) == 0:\n",
    "        return None\n",
    "    results_list = []\n",
    "    for r in results:\n",
    "        results_list.append({\"intent\": intents[r[0]], \"probability\": str(r[1])})\n",
    "    return results_list[0]\n",
    "\n",
    "\n",
    "def get_response(ints: list, config: dict) -> str:\n",
    "    \"\"\"\n",
    "    Getting random answer for specific intent.\n",
    "    :param ints: all intentions.\n",
    "    :param config: bot config.\n",
    "    :return: random response.\n",
    "    \"\"\"\n",
    "    intent = ints[\"intent\"]\n",
    "    responses = config['intents'][intent][\"responses\"]\n",
    "    return random.choice(responses)\n",
    "\n",
    "\n",
    "def chat_bot_response(msg: str) -> str:\n",
    "    \"\"\"\n",
    "    Getting chat bot answer for user's text.\n",
    "    :param msg: user's text.\n",
    "    :return: chat bot response for user's text.\n",
    "    \"\"\"\n",
    "    ints = predict_intent(msg, model)\n",
    "    res = get_response(ints, config)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZ-YETO69tcf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "EQNFz8d0JnTR",
    "outputId": "55d312c5-cf24-4799-c776-ea8388e034e3",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "chat_bot_response(\"Прииивет!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chat_model_refactor.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a968228e1143a5a9e8a40a1729cf09fa367620d0c06a4954f243a786287a2cb7"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "2647ea34e536f865ab67ff9ddee7fd78773d956cec0cab53c79b32cd10da5d83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}